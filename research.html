<!DOCTYPE html>
<html>

  <head>

    <!-- TODO: Update the title of your website -->
    <title>Kate Pearce's Personal Website</title>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="index.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
    <script src="/script.js" defer></script>
  </head>

  <body>
    <nav class="navbar">
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="teachingandvolunteering.html">Volunteering and Teaching</a></li>
        <li><a href="personalprojects.html">Personal Projects</a></li>
        <li><a href="reflections.html">Reflections</a></li>
      </ul>
    </nav>

    <div class="container2">
      <h1>
        Research
      </h1>
    </div>




<div class="flex-container">
      <div class="container">
        <h2>
          A Comparative Study of Transformer-Based Language Models on Extractive Question Answering
        </h2>

        <p>This past summer, I started dipping my toes into the world of natural language processing, and worked on a project in the field of extractive question answering.</p>
        <p>My peers and I first conducted experiments to analyze the effectiveness of pre-existing pretrained language models on the task of extractive question answering (e.g. BERT, ConvBERT, and more), then proposed an original BERT-BiLSTM model architecture that improved the model's performance by using an additional layer of bidirectionality.</p>
        <p>You can view the paper <a href="https://arxiv.org/abs/2110.03142">here!</a></p>
      </div>



    <div class="container2">
      <h2>
        An Intelligent Tutoring System to Predict Learning Styles: MIT THINK Scholars Program
      </h2>



        <p>In January 2021, my proposal to develop an intelligent tutoring system that would not only predict student knowledge states, but also their learning preferences, was one of six out of over four hundred proposals selected as a finalist in the MIT THINK competition.</p>

        <p>Throughout the spring semester, I collaborated with MIT students and faculty to bring my idea to life and fleshing out four main modules: the student model, which determines what a student does and doesn't know; the domain model, which holds the mathematical knowledge of the system; the tutor model, which determines how to teach content to the student; and the user interface model, which provides the connection between student and ITS.</p>
        <p>You can read more <a href="think.mit.edu">here!</a></p>


    </div>

    <div class="container">
      <h2>
        Text Analysis Lab at the University of Arkansas
      </h2>





        <p>This July, I started interning with Dr. Gauch at the Text Analysis Lab at the University of Arkansas. We are currently doing work with sentiment analysis to detect fake news and misinformation; I am personally working on experimenting with different word embedding techniques to improve the accuracy of the model.</p>
        <p>More coming soon!</p>


    </div>
    </div>



  </body>
  <footer class="container2">Created by Kate Pearce, 2021</footer>

</html>
